{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0Uv5IYvNTC6"
      },
      "source": [
        "Here you guys are going to need to integrate the data I pulled from NASA firms into your overall master CSV,\n",
        "\n",
        "You guys must write a script that takes a Fire coordinate and finds the closest weather station.\n",
        "\n",
        "You guys must generate \"No Fire\" rows random dates/locations where no fire occurred so the model has something to compare against.\n",
        "\n",
        "FINAL NOTE: Whatever columns you include, record on the google doc in this same folder listing where it came from and what the values\n",
        "in the column record, it will help with documentation and help the model team along as well'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGEHRDyMLinS",
        "outputId": "99e9bbd4-4ae7-425d-8593-b7523803f934"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: meteostat<2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dsml/lib/python3.11/site-packages (1.7.6)\n",
            "Requirement already satisfied: pandas>=2 in /opt/homebrew/Caskroom/miniforge/base/envs/dsml/lib/python3.11/site-packages (from meteostat<2.0) (2.3.3)\n",
            "Requirement already satisfied: pytz in /opt/homebrew/Caskroom/miniforge/base/envs/dsml/lib/python3.11/site-packages (from meteostat<2.0) (2023.4)\n",
            "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniforge/base/envs/dsml/lib/python3.11/site-packages (from meteostat<2.0) (2.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniforge/base/envs/dsml/lib/python3.11/site-packages (from pandas>=2->meteostat<2.0) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniforge/base/envs/dsml/lib/python3.11/site-packages (from pandas>=2->meteostat<2.0) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniforge/base/envs/dsml/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2->meteostat<2.0) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"meteostat<2.0\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from meteostat import Stations, Daily, Point #this is used to help you guys pull all the data from the relevant stations within California"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "1mLYQgffF-q4",
        "outputId": "dbb63bab-89f6-4be4-c8de-546473e1fecb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned data saved to: fire_archive_cleaned.csv\n",
            "Final row count: 574297\n"
          ]
        }
      ],
      "source": [
        "# This script is used to clean the Archive FIRMS Data\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIG\n",
        "# -----------------------------\n",
        "INPUT_CSV = \"fire_archive_SV-C2_710372.csv\"\n",
        "OUTPUT_CSV = \"fire_archive_cleaned.csv\"\n",
        "\n",
        "# -----------------------------\n",
        "# LOAD DATA\n",
        "# -----------------------------\n",
        "df = pd.read_csv(INPUT_CSV)\n",
        "\n",
        "# -----------------------------\n",
        "# STANDARDIZE COLUMN NAMES\n",
        "# -----------------------------\n",
        "df.columns = df.columns.str.lower().str.strip()\n",
        "\n",
        "# -----------------------------\n",
        "# DROP DUPLICATES\n",
        "# -----------------------------\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# -----------------------------\n",
        "# QUALITY FILTERS\n",
        "# -----------------------------\n",
        "# Keep only nominal & high-confidence detections\n",
        "if \"confidence\" in df.columns:\n",
        "    df = df[df[\"confidence\"].isin([\"n\", \"h\"])]\n",
        "\n",
        "# Remove obviously bad FRP values\n",
        "if \"frp\" in df.columns:\n",
        "    df = df[df[\"frp\"] > 0]\n",
        "\n",
        "# Remove bad brightness values\n",
        "if \"bright_ti4\" in df.columns:\n",
        "    df = df[df[\"bright_ti4\"] > 0]\n",
        "\n",
        "# -----------------------------\n",
        "# DATE / TIME HANDLING\n",
        "# -----------------------------\n",
        "# Convert acq_date to datetime\n",
        "df[\"acq_date\"] = pd.to_datetime(df[\"acq_date\"], errors=\"coerce\")\n",
        "\n",
        "# Convert acq_time to HH:MM format\n",
        "df[\"acq_time\"] = df[\"acq_time\"].astype(str).str.zfill(4)\n",
        "df[\"acq_hour\"] = df[\"acq_time\"].str[:2].astype(int)\n",
        "df[\"acq_minute\"] = df[\"acq_time\"].str[2:].astype(int)\n",
        "\n",
        "# Combine into a single timestamp\n",
        "df[\"timestamp_utc\"] = (\n",
        "    df[\"acq_date\"]\n",
        "    + pd.to_timedelta(df[\"acq_hour\"], unit=\"h\")\n",
        "    + pd.to_timedelta(df[\"acq_minute\"], unit=\"m\")\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# GEOGRAPHIC SANITY CHECKS\n",
        "# -----------------------------\n",
        "df = df[\n",
        "    (df[\"latitude\"].between(-90, 90)) &\n",
        "    (df[\"longitude\"].between(-180, 180))\n",
        "]\n",
        "\n",
        "# -----------------------------\n",
        "# DROP UNUSED / REDUNDANT COLUMNS\n",
        "# -----------------------------\n",
        "columns_to_drop = [\n",
        "    \"acq_time\",        # replaced by timestamp\n",
        "    \"bright_ti5\",      # often redundant\n",
        "    \"version\",         # metadata\n",
        "    \"confidence\"       # already filtered\n",
        "]\n",
        "\n",
        "df = df.drop(columns=[c for c in columns_to_drop if c in df.columns])\n",
        "\n",
        "# -----------------------------\n",
        "# SORT + RESET INDEX\n",
        "# -----------------------------\n",
        "df = df.sort_values(\"timestamp_utc\").reset_index(drop=True)\n",
        "\n",
        "# -----------------------------\n",
        "# SAVE CLEAN FILE\n",
        "# -----------------------------\n",
        "df.to_csv(OUTPUT_CSV, index=False)\n",
        "\n",
        "print(f\"Cleaned data saved to: {OUTPUT_CSV}\")\n",
        "print(f\"Final row count: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "dnhObAlQGJaa",
        "outputId": "6a9aca32-ca6f-46d8-944c-9cad4bc39092"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NRT cleaned data saved to: fire_nrt_cleaned.csv\n",
            "Final row count: 4652\n"
          ]
        }
      ],
      "source": [
        "# Because the NRT data is more sensitive to clean, this cell is dedicated\n",
        "# to the NRT dataset.\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIG\n",
        "# -----------------------------\n",
        "INPUT_CSV = \"fire_nrt_SV-C2_710372.csv\"\n",
        "OUTPUT_CSV = \"fire_nrt_cleaned.csv\"\n",
        "\n",
        "# -----------------------------\n",
        "# LOAD DATA\n",
        "# -----------------------------\n",
        "df = pd.read_csv(INPUT_CSV)\n",
        "df.columns = df.columns.str.lower().str.strip()\n",
        "\n",
        "# -----------------------------\n",
        "# DROP DUPLICATES\n",
        "# -----------------------------\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# -----------------------------\n",
        "# BASIC SANITY CHECKS (NOT AGGRESSIVE)\n",
        "# -----------------------------\n",
        "df = df[\n",
        "    (df[\"latitude\"].between(-90, 90)) &\n",
        "    (df[\"longitude\"].between(-180, 180))\n",
        "]\n",
        "\n",
        "# -----------------------------\n",
        "# FRP / BRIGHTNESS SANITY\n",
        "# (flag, don't drop)\n",
        "# -----------------------------\n",
        "df[\"frp_valid\"] = True\n",
        "df.loc[df[\"frp\"] <= 0, \"frp_valid\"] = False\n",
        "\n",
        "if \"bright_ti4\" in df.columns:\n",
        "    df[\"bright_valid\"] = True\n",
        "    df.loc[df[\"bright_ti4\"] <= 0, \"bright_valid\"] = False\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIDENCE HANDLING\n",
        "# -----------------------------\n",
        "# Convert confidence to numeric scale\n",
        "confidence_map = {\n",
        "    \"l\": 0,\n",
        "    \"n\": 1,\n",
        "    \"h\": 2\n",
        "}\n",
        "\n",
        "if \"confidence\" in df.columns:\n",
        "    df[\"confidence_level\"] = df[\"confidence\"].map(confidence_map)\n",
        "else:\n",
        "    df[\"confidence_level\"] = None\n",
        "\n",
        "# -----------------------------\n",
        "# DATE / TIME HANDLING\n",
        "# -----------------------------\n",
        "df[\"acq_date\"] = pd.to_datetime(df[\"acq_date\"], errors=\"coerce\")\n",
        "\n",
        "df[\"acq_time\"] = df[\"acq_time\"].astype(str).str.zfill(4)\n",
        "df[\"acq_hour\"] = df[\"acq_time\"].str[:2].astype(int)\n",
        "df[\"acq_minute\"] = df[\"acq_time\"].str[2:].astype(int)\n",
        "\n",
        "df[\"timestamp_utc\"] = (\n",
        "    df[\"acq_date\"]\n",
        "    + pd.to_timedelta(df[\"acq_hour\"], unit=\"h\")\n",
        "    + pd.to_timedelta(df[\"acq_minute\"], unit=\"m\")\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# SATELLITE / INSTRUMENT FLAGS\n",
        "# -----------------------------\n",
        "if \"instrument\" in df.columns:\n",
        "    df[\"is_viirs\"] = df[\"instrument\"].str.contains(\"VIIRS\", na=False)\n",
        "\n",
        "# -----------------------------\n",
        "# DROP ONLY TRUE REDUNDANCY\n",
        "# -----------------------------\n",
        "columns_to_drop = [\n",
        "    \"acq_time\",   # replaced by timestamp\n",
        "]\n",
        "\n",
        "df = df.drop(columns=[c for c in columns_to_drop if c in df.columns])\n",
        "\n",
        "# -----------------------------\n",
        "# SORT + SAVE\n",
        "# -----------------------------\n",
        "df = df.sort_values(\"timestamp_utc\").reset_index(drop=True)\n",
        "df.to_csv(OUTPUT_CSV, index=False)\n",
        "\n",
        "print(f\"NRT cleaned data saved to: {OUTPUT_CSV}\")\n",
        "print(f\"Final row count: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g5xgY4bQGbhZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-reference saved to: firms_calfire_cross_reference.csv\n",
            "Total matches found: 39\n"
          ]
        }
      ],
      "source": [
        "# Cross-checking FIRMS Archive Data with 5 most recent fires from CAL FIRE\n",
        "''' (I didn't know how to convert the data from CAL FIRE into a CSV file,\n",
        "so I wrote it by hand. I added it to the Raw Data folder on the Drive.\n",
        "The title is \"cal_fire_top_5.csv\")\n",
        "-Arjun '''\n",
        "\n",
        "# Import geodesic for easier distance calculation\n",
        "from geopy.distance import geodesic\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIG\n",
        "# -----------------------------\n",
        "FIRMS_CSV = \"fire_archive_cleaned.csv\"\n",
        "CALFIRE_CSV = \"cal_fire_top_5.csv\"\n",
        "OUTPUT_CSV = \"firms_calfire_cross_reference.csv\"\n",
        "\n",
        "MAX_DISTANCE_KM = 10      # spatial tolerance\n",
        "TIME_WINDOW_DAYS = 1      # ± days around start date\n",
        "\n",
        "# -----------------------------\n",
        "# LOAD DATA\n",
        "# -----------------------------\n",
        "firms = pd.read_csv(FIRMS_CSV, parse_dates=[\"timestamp_utc\"])\n",
        "calfire = pd.read_csv(CALFIRE_CSV, parse_dates=[\"start_date\"])\n",
        "\n",
        "results = []\n",
        "\n",
        "# -----------------------------\n",
        "# CROSS-REFERENCE\n",
        "# -----------------------------\n",
        "for _, fire in calfire.iterrows():\n",
        "    fire_point = (fire[\"latitude\"], fire[\"longitude\"])\n",
        "\n",
        "    # time window\n",
        "    start = fire[\"start_date\"] - pd.Timedelta(days=TIME_WINDOW_DAYS)\n",
        "    end = fire[\"start_date\"] + pd.Timedelta(days=TIME_WINDOW_DAYS)\n",
        "\n",
        "    # FIRMS detections in time window\n",
        "    firms_subset = firms[\n",
        "        (firms[\"timestamp_utc\"] >= start) &\n",
        "        (firms[\"timestamp_utc\"] <= end)\n",
        "    ]\n",
        "\n",
        "    for _, f in firms_subset.iterrows():\n",
        "        firms_point = (f[\"latitude\"], f[\"longitude\"])\n",
        "        distance_km = geodesic(fire_point, firms_point).km\n",
        "\n",
        "        if distance_km <= MAX_DISTANCE_KM:\n",
        "            results.append({\n",
        "                \"incident_name\": fire[\"incident_name\"],\n",
        "                \"incident_lat\": fire[\"latitude\"],\n",
        "                \"incident_lon\": fire[\"longitude\"],\n",
        "                \"firms_lat\": f[\"latitude\"],\n",
        "                \"firms_lon\": f[\"longitude\"],\n",
        "                \"distance_km\": round(distance_km, 2),\n",
        "                \"firms_time\": f[\"timestamp_utc\"]\n",
        "            })\n",
        "\n",
        "# -----------------------------\n",
        "# SAVE RESULTS\n",
        "# -----------------------------\n",
        "matches_df = pd.DataFrame(results)\n",
        "matches_df.to_csv(OUTPUT_CSV, index=False)\n",
        "\n",
        "print(f\"Cross-reference saved to: {OUTPUT_CSV}\")\n",
        "print(f\"Total matches found: {len(matches_df)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yPERbUfmHPGL"
      },
      "outputs": [],
      "source": [
        "# This script adds the seed zones into the cleaned up FIRMS Archive File\n",
        "''' I got a bit of help for this because I'm not the best with geopandas.\n",
        "If anything seems off after running this script, please let me know and\n",
        "I'll fix it asap.\n",
        "-Arjun '''\n",
        "\n",
        "# Importing geopandas library due to geojson file of seed zones\n",
        "import geopandas as gpd\n",
        "\n",
        "# -----------------------------\n",
        "# LOAD DATA\n",
        "# -----------------------------\n",
        "firms = pd.read_csv(\"fire_archive_cleaned.csv\")\n",
        "\n",
        "# Convert FIRMS to GeoDataFrame\n",
        "firms_gdf = gpd.GeoDataFrame(\n",
        "    firms,\n",
        "    geometry=gpd.points_from_xy(firms.longitude, firms.latitude),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "# Load seed zone polygons\n",
        "seed_zones = gpd.read_file(\"California_Seed_Zones_3280520806235389701.geojson\")\n",
        "\n",
        "# Ensure same CRS\n",
        "seed_zones = seed_zones.to_crs(firms_gdf.crs)\n",
        "\n",
        "# -----------------------------\n",
        "# SPATIAL JOIN\n",
        "# -----------------------------\n",
        "firms_with_seed = gpd.sjoin(\n",
        "    firms_gdf,\n",
        "    seed_zones[[\"SEED_ZONE\", \"REGION\", \"SUBREGION\", \"SUBZONE\", \"geometry\"]],\n",
        "    how=\"left\",\n",
        "    predicate=\"within\"\n",
        ")\n",
        "\n",
        "# Drop geometry for CSV output\n",
        "firms_with_seed = firms_with_seed.drop(columns=\"geometry\")\n",
        "\n",
        "firms_with_seed.to_csv(\"fire_firms_with_seed_zones.csv\", index=False)\n",
        "\n",
        "# If you want to locally download the finished file, delete the hashtag below:\n",
        "# files.download(\"fire_firms_with_seed_zones.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fires: (574297, 23) no_fire: (287148, 8)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>date</th>\n",
              "      <th>fire</th>\n",
              "      <th>SEED_ZONE</th>\n",
              "      <th>REGION</th>\n",
              "      <th>SUBREGION</th>\n",
              "      <th>SUBZONE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>36.982410</td>\n",
              "      <td>-117.899236</td>\n",
              "      <td>2023-07-20</td>\n",
              "      <td>0</td>\n",
              "      <td>981</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>40.823712</td>\n",
              "      <td>-120.702148</td>\n",
              "      <td>2025-09-07</td>\n",
              "      <td>0</td>\n",
              "      <td>732</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>39.382897</td>\n",
              "      <td>-121.096158</td>\n",
              "      <td>2020-07-16</td>\n",
              "      <td>0</td>\n",
              "      <td>525</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>34.619255</td>\n",
              "      <td>-117.888680</td>\n",
              "      <td>2022-01-09</td>\n",
              "      <td>0</td>\n",
              "      <td>982</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>37.065257</td>\n",
              "      <td>-118.419327</td>\n",
              "      <td>2021-11-09</td>\n",
              "      <td>0</td>\n",
              "      <td>784</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    latitude   longitude       date  fire SEED_ZONE REGION SUBREGION SUBZONE\n",
              "0  36.982410 -117.899236 2023-07-20     0       981      9         8       1\n",
              "1  40.823712 -120.702148 2025-09-07     0       732      7         3       2\n",
              "2  39.382897 -121.096158 2020-07-16     0       525      5         2       5\n",
              "3  34.619255 -117.888680 2022-01-09     0       982      9         8       2\n",
              "4  37.065257 -118.419327 2021-11-09     0       784      7         8       4"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "Used seed zone data to randomize points in California with no fire and at least 5KM away from the nearest fire that day. First converted lon,lat into metric units since it works better this way. Ratio for fire rows and no-fire rows: 1:0.5\n",
        "-Will\n",
        "'''\n",
        "\n",
        "from shapely.geometry import Point\n",
        "from pyproj import Transformer\n",
        "import random\n",
        "\n",
        "fires = pd.read_csv(\"fire_firms_with_seed_zones.csv\")\n",
        "fires[\"date\"] = pd.to_datetime(fires[\"acq_date\"]).dt.normalize()\n",
        "fires = fires.drop(columns=[\"acq_date\"], errors=\"ignore\")\n",
        "\n",
        "fires[\"fire\"] = 1\n",
        "\n",
        "#load seed zones\n",
        "seed_zones = gpd.read_file(\"California_Seed_Zones_3280520806235389701.geojson\")\n",
        "\n",
        "seed_cols = [\"SEED_ZONE\", \"REGION\", \"SUBREGION\", \"SUBZONE\", \"geometry\"]\n",
        "seed_zones = seed_zones[seed_cols].to_crs(epsg=4326)\n",
        "\n",
        "seed_zones_5070 = seed_zones.to_crs(epsg=5070)\n",
        "seed_zones_5070[\"area\"] = seed_zones_5070.geometry.area\n",
        "\n",
        "zone_weights = seed_zones_5070[\"area\"].to_numpy()\n",
        "\n",
        "# projections\n",
        "to_5070 = Transformer.from_crs(\"EPSG:4326\", \"EPSG:5070\", always_xy=True)\n",
        "to_4326 = Transformer.from_crs(\"EPSG:5070\", \"EPSG:4326\", always_xy=True)\n",
        "\n",
        "def lonlat_to_5070(lon, lat):\n",
        "    return to_5070.transform(lon, lat)\n",
        "\n",
        "def xy5070_to_lonlat(x, y):\n",
        "    return to_4326.transform(x,y)\n",
        "\n",
        "# add projected cords for fire rows\n",
        "fires[\"x\"], fires[\"y\"] = zip(*fires.apply(\n",
        "    lambda r: lonlat_to_5070(r[\"longitude\"], r[\"latitude\"]),\n",
        "    axis=1\n",
        "))\n",
        "\n",
        "def random_point(poly):\n",
        "    minx, miny, maxx, maxy = poly.bounds\n",
        "    while True:\n",
        "        p = Point(random.uniform(minx, maxx), random.uniform(miny, maxy))\n",
        "        if poly.contains(p):\n",
        "            return p\n",
        "        \n",
        "n = int(len(fires) * 0.5)\n",
        "no_fire_rows = []\n",
        "\n",
        "fires_by_date = fires.groupby(\"date\")\n",
        "unique_dates = fires[\"date\"].dropna().unique()\n",
        "\n",
        "MIN_DIST_M = 5_000\n",
        "\n",
        "while len(no_fire_rows) < n:\n",
        "    #choose random day from fire dataset\n",
        "    date = random.choice(unique_dates)\n",
        "    day_fire = fires_by_date.get_group(date)\n",
        "\n",
        "    #choose seed zone\n",
        "    zone_idx = random.choices(range(len(seed_zones_5070)), weights=zone_weights, k=1)[0]\n",
        "    zone = seed_zones_5070.iloc[zone_idx]\n",
        "\n",
        "    #sample point inside chosen zone\n",
        "    p_xy = random_point(zone.geometry)\n",
        "    x, y = p_xy.x, p_xy.y\n",
        "\n",
        "    #convert back to lon/lat\n",
        "    lon, lat = xy5070_to_lonlat(x, y)\n",
        "\n",
        "    # distance to nearest same-day fire\n",
        "    dist = np.sqrt((day_fire[\"x\"] - x)**2 + (day_fire[\"y\"] - y)**2)\n",
        "\n",
        "    if dist.min() > MIN_DIST_M:\n",
        "        no_fire_rows.append({\n",
        "            \"latitude\": lat,\n",
        "            \"longitude\": lon,\n",
        "            \"date\": date,\n",
        "            \"fire\": 0,\n",
        "            \"SEED_ZONE\": zone[\"SEED_ZONE\"],\n",
        "            \"REGION\": zone[\"REGION\"],\n",
        "            \"SUBREGION\": zone[\"SUBREGION\"],\n",
        "            \"SUBZONE\": zone[\"SUBZONE\"],\n",
        "        })\n",
        "\n",
        "no_fire = pd.DataFrame(no_fire_rows)\n",
        "print(\"fires:\", fires.shape, \"no_fire:\", no_fire.shape)\n",
        "no_fire.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "master: (861445, 23)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fire</th>\n",
              "      <th>date</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>SEED_ZONE</th>\n",
              "      <th>REGION</th>\n",
              "      <th>SUBREGION</th>\n",
              "      <th>SUBZONE</th>\n",
              "      <th>brightness</th>\n",
              "      <th>scan</th>\n",
              "      <th>...</th>\n",
              "      <th>bright_t31</th>\n",
              "      <th>frp</th>\n",
              "      <th>daynight</th>\n",
              "      <th>type</th>\n",
              "      <th>acq_hour</th>\n",
              "      <th>acq_minute</th>\n",
              "      <th>timestamp_utc</th>\n",
              "      <th>index_right</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2020-01-01</td>\n",
              "      <td>37.54193</td>\n",
              "      <td>-120.73184</td>\n",
              "      <td>962.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>323.38</td>\n",
              "      <td>0.34</td>\n",
              "      <td>...</td>\n",
              "      <td>267.36</td>\n",
              "      <td>1.38</td>\n",
              "      <td>N</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2020-01-01 09:11:00</td>\n",
              "      <td>41.0</td>\n",
              "      <td>-2.140218e+06</td>\n",
              "      <td>1.891557e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2020-01-01</td>\n",
              "      <td>36.35637</td>\n",
              "      <td>-114.91145</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>310.17</td>\n",
              "      <td>0.50</td>\n",
              "      <td>...</td>\n",
              "      <td>273.50</td>\n",
              "      <td>1.70</td>\n",
              "      <td>N</td>\n",
              "      <td>2.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2020-01-01 09:11:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1.670516e+06</td>\n",
              "      <td>1.645415e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>2020-01-01</td>\n",
              "      <td>37.54229</td>\n",
              "      <td>-120.73358</td>\n",
              "      <td>962.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>322.64</td>\n",
              "      <td>0.34</td>\n",
              "      <td>...</td>\n",
              "      <td>266.92</td>\n",
              "      <td>1.75</td>\n",
              "      <td>N</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2020-01-01 09:11:00</td>\n",
              "      <td>41.0</td>\n",
              "      <td>-2.140355e+06</td>\n",
              "      <td>1.891635e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>2020-01-01</td>\n",
              "      <td>33.09716</td>\n",
              "      <td>-116.12562</td>\n",
              "      <td>986.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>296.45</td>\n",
              "      <td>0.52</td>\n",
              "      <td>...</td>\n",
              "      <td>275.92</td>\n",
              "      <td>0.80</td>\n",
              "      <td>N</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2020-01-01 09:12:00</td>\n",
              "      <td>87.0</td>\n",
              "      <td>-1.852828e+06</td>\n",
              "      <td>1.311066e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2020-01-01</td>\n",
              "      <td>33.61496</td>\n",
              "      <td>-117.82188</td>\n",
              "      <td>995.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>303.05</td>\n",
              "      <td>0.43</td>\n",
              "      <td>...</td>\n",
              "      <td>282.23</td>\n",
              "      <td>1.09</td>\n",
              "      <td>N</td>\n",
              "      <td>2.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2020-01-01 09:12:00</td>\n",
              "      <td>79.0</td>\n",
              "      <td>-1.993187e+06</td>\n",
              "      <td>1.401808e+06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 23 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   fire       date  latitude  longitude SEED_ZONE REGION SUBREGION SUBZONE  \\\n",
              "0     1 2020-01-01  37.54193 -120.73184     962.0    9.0       6.0     2.0   \n",
              "1     1 2020-01-01  36.35637 -114.91145       NaN    NaN       NaN     NaN   \n",
              "2     1 2020-01-01  37.54229 -120.73358     962.0    9.0       6.0     2.0   \n",
              "3     1 2020-01-01  33.09716 -116.12562     986.0    9.0       8.0     6.0   \n",
              "4     1 2020-01-01  33.61496 -117.82188     995.0    9.0       9.0     5.0   \n",
              "\n",
              "   brightness  scan  ...  bright_t31   frp daynight  type  acq_hour  \\\n",
              "0      323.38  0.34  ...      267.36  1.38        N   0.0       9.0   \n",
              "1      310.17  0.50  ...      273.50  1.70        N   2.0       9.0   \n",
              "2      322.64  0.34  ...      266.92  1.75        N   0.0       9.0   \n",
              "3      296.45  0.52  ...      275.92  0.80        N   0.0       9.0   \n",
              "4      303.05  0.43  ...      282.23  1.09        N   2.0       9.0   \n",
              "\n",
              "  acq_minute        timestamp_utc  index_right             x             y  \n",
              "0       11.0  2020-01-01 09:11:00         41.0 -2.140218e+06  1.891557e+06  \n",
              "1       11.0  2020-01-01 09:11:00          NaN -1.670516e+06  1.645415e+06  \n",
              "2       11.0  2020-01-01 09:11:00         41.0 -2.140355e+06  1.891635e+06  \n",
              "3       12.0  2020-01-01 09:12:00         87.0 -1.852828e+06  1.311066e+06  \n",
              "4       12.0  2020-01-01 09:12:00         79.0 -1.993187e+06  1.401808e+06  \n",
              "\n",
              "[5 rows x 23 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "master_draft = pd.concat([fires, no_fire], ignore_index=True, sort=False)\n",
        "\n",
        "front = [\"fire\", \"date\", \"latitude\", \"longitude\", \"SEED_ZONE\", \"REGION\", \"SUBREGION\", \"SUBZONE\"]\n",
        "cols = front + [c for c in master_draft.columns if c not in front]\n",
        "master_draft = master_draft[cols]\n",
        "\n",
        "master.to_csv(\"master_draft.csv\", index=False)\n",
        "print(\"master:\", master.shape)\n",
        "master.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DtypeWarning: Columns (0: satellite, 1: instrument, 2: daynight, 3: timestamp_utc) have mixed types. Specify dtype option on import or set low_memory=False.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: master_with_stations.csv rows: 861445\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>nearest_station_id</th>\n",
              "      <th>nearest_station_distance_km</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>37.54193</td>\n",
              "      <td>-120.73184</td>\n",
              "      <td>KMOD0</td>\n",
              "      <td>21.715945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>36.35637</td>\n",
              "      <td>-114.91145</td>\n",
              "      <td>72380</td>\n",
              "      <td>178.718331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>37.54229</td>\n",
              "      <td>-120.73358</td>\n",
              "      <td>KMOD0</td>\n",
              "      <td>21.560266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33.09716</td>\n",
              "      <td>-116.12562</td>\n",
              "      <td>KL080</td>\n",
              "      <td>25.582935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33.61496</td>\n",
              "      <td>-117.82188</td>\n",
              "      <td>KSNA0</td>\n",
              "      <td>8.000062</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   latitude  longitude nearest_station_id  nearest_station_distance_km\n",
              "0  37.54193 -120.73184              KMOD0                    21.715945\n",
              "1  36.35637 -114.91145              72380                   178.718331\n",
              "2  37.54229 -120.73358              KMOD0                    21.560266\n",
              "3  33.09716 -116.12562              KL080                    25.582935\n",
              "4  33.61496 -117.82188              KSNA0                     8.000062"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from meteostat import Stations, Daily\n",
        "from datetime import datetime\n",
        "\n",
        "# COMPILING NEARTST WEATHER STATIONS\n",
        "\n",
        "# CONFIG\n",
        "INPUT_CSV = \"master_draft.csv\"\n",
        "OUTPUT_CSV = \"master_with_stations.csv\"\n",
        "\n",
        "# load master csv\n",
        "master_draft = pd.read_csv(INPUT_CSV, parse_dates=[\"date\"])\n",
        "master_draft[\"date\"] = pd.to_datetime(master_draft[\"date\"]).dt.date #join key\n",
        "\n",
        "# GET STATION CATALOG (CALIFORNIA)\n",
        "stations = (\n",
        "    Stations()\n",
        "    .region('US', 'CA')\n",
        "    .inventory('daily')\n",
        "    .fetch()\n",
        ")\n",
        "\n",
        "stations = stations.reset_index()\n",
        "if \"id\" in stations.columns:\n",
        "    stations = stations.rename(columns={\"id\": \"station_id\"})\n",
        "elif \"index\" in stations.columns:\n",
        "    stations = stations.rename(columns={\"index\": \"station_id\"})                                        \n",
        "\n",
        "stations = stations.rename(\n",
        "    columns={\n",
        "        \"latitude\": \"station_lat\",\n",
        "        \"longitude\": \"station_lon\",\n",
        "    }\n",
        ")\n",
        "\n",
        "stations = stations[[\"station_id\", \"station_lat\", \"station_lon\"]]\n",
        "# HAVERSINE DISTANCE (KM)\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    \"\"\"\n",
        "    Great-circle distance between two points (deg) in km.\n",
        "    In simpler terms, gets the distance between two points in a sphere\n",
        "    lat2/lon2 can be vector; returns numpy array.\n",
        "    \"\"\"\n",
        "    R = 6371.0\n",
        "    lat1 = np.radians(lat1)\n",
        "    lon1 = np.radians(lon1)\n",
        "    lat2 = np.radians(lat2)\n",
        "    lon2 = np.radians(lon2)\n",
        "\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "\n",
        "    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n",
        "    c = 2 * np.arcsin(np.sqrt(a))\n",
        "\n",
        "    return R * c\n",
        "\n",
        "# ASSIGN NEAREST STATION TO UNIQUE LOCATIONS\n",
        "locs = master_draft[[\"latitude\", \"longitude\"]].drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "def find_nearest_station(row):\n",
        "  \"\"\"\n",
        "  Finds the nearest station by having the haversine return a numpy array\n",
        "  with all the distances, and then simply choosing the minimum distance\n",
        "\n",
        "  \"\"\"\n",
        "  lat = row[\"latitude\"]\n",
        "  lon = row[\"longitude\"]\n",
        "  dists = haversine(lat, lon, stations[\"station_lat\"].values, stations[\"station_lon\"].values)\n",
        "  idx = int(dists.argmin())\n",
        "  return pd.Series(\n",
        "        {\n",
        "            \"nearest_station_id\": stations.loc[idx, \"station_id\"],\n",
        "            \"nearest_station_distance_km\": float(dists[idx]),\n",
        "        }\n",
        "    )\n",
        "\n",
        "nearest_info = locs.apply(find_nearest_station, axis=1)\n",
        "locs = pd.concat([locs, nearest_info], axis=1)\n",
        "\n",
        "master_draft = master_draft.merge(\n",
        "    locs,\n",
        "    on=[\"latitude\", \"longitude\"],\n",
        "    how=\"left\",\n",
        ")\n",
        "\n",
        "master_draft.to_csv(OUTPUT_CSV, index=False)\n",
        "print(\"Saved:\", OUTPUT_CSV, \"rows:\", len(master))\n",
        "master_draft[\n",
        "    [\"latitude\", \"longitude\", \"nearest_station_id\", \"nearest_station_distance_km\"]].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DtypeWarning: Columns (0: satellite, 1: instrument, 2: daynight, 3: timestamp_utc) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "Warning: Cannot load daily/2020/KTOA0.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2020/69014.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2021/69014.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2022/69014.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2023/69014.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2024/69014.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2020/72004.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2021/72004.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2022/72004.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2023/72004.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2024/72004.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2025/72004.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2020/ZGK9P.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2021/ZGK9P.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2020/8RXA6.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2021/8RXA6.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2020/72289.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2021/72289.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2022/72289.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2023/72289.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2020/BFY6K.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2021/BFY6K.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2020/NG9G1.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2021/NG9G1.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2020/74506.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2021/74506.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2022/74506.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2023/74506.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2024/74506.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2020/74509.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2020/KO050.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2020/Q5MJ7.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2021/Q5MJ7.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2024/KCZZ0.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2020/69002.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2021/69002.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2022/69002.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2023/69002.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2024/69002.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2020/LQO8Z.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2021/LQO8Z.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2020/M44D9.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2021/M44D9.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2020/72390.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2021/72390.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2022/72390.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2023/72390.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2024/72390.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2025/72390.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2020/QX6JS.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2021/QX6JS.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2020/K1O50.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2021/K1O50.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2022/K1O50.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2023/K1O50.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2024/K1O50.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2020/KIYK0.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2021/KIYK0.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2020/FSE8R.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2021/FSE8R.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2020/KSZN0.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2021/KSZN0.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2022/KSZN0.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2023/KSZN0.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2024/KSZN0.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2025/KSZN0.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2020/71D38.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2021/71D38.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2020/72491.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2021/72491.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2022/72491.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2023/72491.csv.gz from https://data.meteostat.net/\n",
            "Warning: Cannot load daily/2024/72491.csv.gz from https://data.meteostat.net/\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: master_with_weather.csv rows: 861445\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fire</th>\n",
              "      <th>date</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>SEED_ZONE</th>\n",
              "      <th>REGION</th>\n",
              "      <th>SUBREGION</th>\n",
              "      <th>SUBZONE</th>\n",
              "      <th>brightness</th>\n",
              "      <th>scan</th>\n",
              "      <th>...</th>\n",
              "      <th>wx_tavg_c</th>\n",
              "      <th>wx_tmin_c</th>\n",
              "      <th>wx_tmax_c</th>\n",
              "      <th>wx_prcp_mm</th>\n",
              "      <th>snow</th>\n",
              "      <th>wx_wdir_deg</th>\n",
              "      <th>wx_wspd_ms</th>\n",
              "      <th>wx_wpgt_kmh</th>\n",
              "      <th>wx_pres_hpa</th>\n",
              "      <th>wx_tsun_min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2020-01-01</td>\n",
              "      <td>37.54193</td>\n",
              "      <td>-120.73184</td>\n",
              "      <td>962.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>323.38</td>\n",
              "      <td>0.34</td>\n",
              "      <td>...</td>\n",
              "      <td>9.2</td>\n",
              "      <td>3.9</td>\n",
              "      <td>15.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>6.0</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>1019.8</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2020-01-01</td>\n",
              "      <td>36.35637</td>\n",
              "      <td>-114.91145</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>310.17</td>\n",
              "      <td>0.50</td>\n",
              "      <td>...</td>\n",
              "      <td>8.7</td>\n",
              "      <td>4.4</td>\n",
              "      <td>13.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>6.3</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>1014.9</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>2020-01-01</td>\n",
              "      <td>37.54229</td>\n",
              "      <td>-120.73358</td>\n",
              "      <td>962.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>322.64</td>\n",
              "      <td>0.34</td>\n",
              "      <td>...</td>\n",
              "      <td>9.2</td>\n",
              "      <td>3.9</td>\n",
              "      <td>15.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>6.0</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>1019.8</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>2020-01-01</td>\n",
              "      <td>33.09716</td>\n",
              "      <td>-116.12562</td>\n",
              "      <td>986.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>296.45</td>\n",
              "      <td>0.52</td>\n",
              "      <td>...</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2020-01-01</td>\n",
              "      <td>33.61496</td>\n",
              "      <td>-117.82188</td>\n",
              "      <td>995.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>303.05</td>\n",
              "      <td>0.43</td>\n",
              "      <td>...</td>\n",
              "      <td>14.8</td>\n",
              "      <td>10.0</td>\n",
              "      <td>19.4</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>5.6</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>1015.9</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 35 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   fire        date  latitude  longitude  SEED_ZONE  REGION  SUBREGION  \\\n",
              "0     1  2020-01-01  37.54193 -120.73184      962.0     9.0        6.0   \n",
              "1     1  2020-01-01  36.35637 -114.91145        NaN     NaN        NaN   \n",
              "2     1  2020-01-01  37.54229 -120.73358      962.0     9.0        6.0   \n",
              "3     1  2020-01-01  33.09716 -116.12562      986.0     9.0        8.0   \n",
              "4     1  2020-01-01  33.61496 -117.82188      995.0     9.0        9.0   \n",
              "\n",
              "   SUBZONE  brightness  scan  ...  wx_tavg_c wx_tmin_c wx_tmax_c  wx_prcp_mm  \\\n",
              "0      2.0      323.38  0.34  ...        9.2       3.9      15.6         0.0   \n",
              "1      NaN      310.17  0.50  ...        8.7       4.4      13.9         0.0   \n",
              "2      2.0      322.64  0.34  ...        9.2       3.9      15.6         0.0   \n",
              "3      6.0      296.45  0.52  ...       <NA>      <NA>      <NA>        <NA>   \n",
              "4      5.0      303.05  0.43  ...       14.8      10.0      19.4        <NA>   \n",
              "\n",
              "   snow wx_wdir_deg  wx_wspd_ms  wx_wpgt_kmh  wx_pres_hpa wx_tsun_min  \n",
              "0  <NA>        <NA>         6.0         <NA>       1019.8        <NA>  \n",
              "1  <NA>        <NA>         6.3         <NA>       1014.9        <NA>  \n",
              "2  <NA>        <NA>         6.0         <NA>       1019.8        <NA>  \n",
              "3  <NA>        <NA>        <NA>         <NA>         <NA>        <NA>  \n",
              "4  <NA>        <NA>         5.6         <NA>       1015.9        <NA>  \n",
              "\n",
              "[5 rows x 35 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "# FETCH OTHER METEOSTAT COLUMNS (DAILY WEATHER)\n",
        "\n",
        "INPUT_CSV = \"master_with_stations.csv\"\n",
        "OUTPUT_CSV = \"master_with_weather.csv\"\n",
        "\n",
        "master = pd.read_csv(INPUT_CSV, parse_dates=[\"date\"])\n",
        "master[\"date\"] = pd.to_datetime(master[\"date\"]).dt.date\n",
        "\n",
        "#datetime bounds\n",
        "start_date = min(master[\"date\"])\n",
        "end_date = max(master[\"date\"])\n",
        "\n",
        "start = datetime.combine(start_date, datetime.min.time())\n",
        "end = datetime.combine(end_date, datetime.min.time())\n",
        "\n",
        "all_daily = []\n",
        "station_ids = master[\"nearest_station_id\"].dropna().unique()\n",
        "\n",
        "for sid in station_ids:\n",
        "    daily = Daily(sid, start, end).fetch()\n",
        "    if daily.empty:\n",
        "        continue\n",
        "\n",
        "    daily = daily.reset_index().rename(columns={\"time\": \"date\"})\n",
        "    daily[\"nearest_station_id\"] = sid\n",
        "\n",
        "    daily = daily[\n",
        "        [\n",
        "            \"nearest_station_id\",\n",
        "            \"date\",\n",
        "            \"tavg\",\n",
        "            \"tmin\",\n",
        "            \"tmax\",\n",
        "            \"prcp\",\n",
        "            \"wdir\",\n",
        "            \"wspd\",\n",
        "            \"wpgt\",\n",
        "            \"pres\",\n",
        "            \"tsun\"\n",
        "        ]\n",
        "    ]\n",
        "    all_daily.append(daily)\n",
        "\n",
        "if not all_daily:\n",
        "    raise RuntimeError(\"No Meteostat daily data for any station in use.\")\n",
        "\n",
        "weather = pd.concat(all_daily, ignore_index=True)\n",
        "\n",
        "# RENAME TO wx_* AND ALIGN TYPES\n",
        "weather = weather.rename(\n",
        "    columns={\n",
        "        \"tavg\": \"wx_tavg_c\",\n",
        "        \"tmin\": \"wx_tmin_c\",\n",
        "        \"tmax\": \"wx_tmax_c\",\n",
        "        \"prcp\": \"wx_prcp_mm\",\n",
        "        \"wdir\": \"wx_wdir_deg\",\n",
        "        \"wspd\": \"wx_wspd_kmh\",\n",
        "        \"wpgt\": \"wx_wpgt_kmh\",\n",
        "        \"pres\": \"wx_pres_hpa\",\n",
        "        \"tsun\": \"wx_tsun_min\"\n",
        "    }\n",
        ")\n",
        "\n",
        "weather[\"date\"] = pd.to_datetime(weather[\"date\"]).dt.date\n",
        "\n",
        "\n",
        "# JOIN WEATHER ONTO FIRMS\n",
        "\n",
        "master_weather = master.merge(\n",
        "    weather,\n",
        "    on=[\"nearest_station_id\", \"date\"],\n",
        "    how=\"left\",\n",
        ")\n",
        "\n",
        "master_weather.to_csv(OUTPUT_CSV, index=False)\n",
        "print(\"Saved:\", OUTPUT_CSV, \"rows:\", len(master_weather))\n",
        "\n",
        "master_weather.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wx_tmax_c      0.136060\n",
            "wx_prcp_mm     0.263890\n",
            "wx_wspd_kmh    0.147582\n",
            "wx_pres_hpa    0.215555\n",
            "wx_tsun_min    1.000000\n",
            "dtype: float64\n",
            "rows: 861445\n",
            "stations used: 153\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Check percentage of NA and NaN just in case\n",
        "'''\n",
        "\n",
        "print(master_weather[[\"wx_tmax_c\",\"wx_prcp_mm\",\"wx_wspd_kmh\", \"wx_pres_hpa\", \"wx_tsun_min\"]].isna().mean())\n",
        "print(\"rows:\", len(master_weather))\n",
        "print(\"stations used:\", master_weather[\"nearest_station_id\"].nunique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DtypeWarning: Columns (0: satellite, 1: instrument, 2: daynight, 3: timestamp_utc) have mixed types. Specify dtype option on import or set low_memory=False.\n"
          ]
        }
      ],
      "source": [
        "''' Got my data from LANDFIRE, extracted .tif of CONUS for Existing Vegetation Type, Height, Cover (denoted evt, evh, evc respectively) and filtered it to only contain data from california (all done on a separate doc since I couldn't just download California data off rip). Code below is mostly done with the help of chat since I've never played with rasters and .tif fiels \n",
        "-Will '''\n",
        "\n",
        "import rasterio\n",
        "\n",
        "df = pd.read_csv(\"master_with_weather.csv\")\n",
        "\n",
        "gdf = gpd.GeoDataFrame(\n",
        "    df,\n",
        "    geometry=gpd.points_from_xy(df[\"longitude\"], df[\"latitude\"]), crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "def add_raster_values(gdf, raster_path, out_col):\n",
        "    with rasterio.open(raster_path) as src:\n",
        "        pts = gdf.to_crs(src.crs)\n",
        "\n",
        "        coords = [(geom.x, geom.y) for geom in pts.geometry]\n",
        "        vals = [v[0] for v in src.sample(coords)]\n",
        "\n",
        "        nodata = src.nodata\n",
        "        if nodata is not None:\n",
        "            vals = [None if (v == nodata) else v for v in vals]\n",
        "\n",
        "        gdf[out_col] = vals\n",
        "    return gdf\n",
        "\n",
        "gdf = add_raster_values(gdf, \"evt_ca.tif\", \"lf_evt\")\n",
        "gdf = add_raster_values(gdf, \"evc_ca.tif\", \"lf_evc\")\n",
        "gdf = add_raster_values(gdf, \"evh_ca.tif\", \"lf_evh\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved master_final.csv (861445, 46)\n"
          ]
        }
      ],
      "source": [
        "'''Merged lookup table adding columns 'lf_evc', 'lf_evh',\n",
        "       'EVT_NAME', 'LFRDB', 'EVT_FUEL', 'EVT_FUEL_N', 'EVT_LF', 'EVT_PHYS',\n",
        "       'EVT_GP', 'EVT_GP_N', 'SAF_SRM', 'EVT_ORDER', 'EVT_CLASS', 'EVT_SBCLS' to provide context on vegetation values \n",
        "-Will \n",
        "'''\n",
        "\n",
        "\n",
        "# Debating if this is the right move. \n",
        "# gdf = gdf.dropna(subset=[\"lf_evt\", \"lf_evc\", \"lf_evh\"]) # input?\n",
        "\n",
        "evt_lookup = pd.read_csv(\"LF2024_EVT.csv\")\n",
        "evt_lookup[\"VALUE\"] = evt_lookup[\"VALUE\"].astype(float)\n",
        "evt_lookup = evt_lookup.drop(columns=[\"R\", \"G\", \"B\", \"RED\", \"GREEN\", \"BLUE\"])\n",
        "gdf = gdf.merge(\n",
        "    evt_lookup,\n",
        "    left_on = \"lf_evt\",\n",
        "    right_on = \"VALUE\",\n",
        "    how=\"left\"\n",
        ")\n",
        "gdf = gdf.drop(columns=[\"lf_evt\", \"VALUE\", \"LFRDB\", \"EVT_ORDER\", \"SAF_SRM\", \"geometry\"])\n",
        "\n",
        "gdf.to_csv(\"master_final.csv\", index=False)\n",
        "print(\"Saved master_final.csv\", gdf.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## New Columns Added\n",
        "lf_evc : (Categorical) % Ground covered by life vegetation (categorical bins defined by LANDFIRE)\n",
        "\n",
        "lf_evh : (Categorical) vertical structure of vegetation\n",
        "\n",
        "EVT_NAME : Name of vegetation type\n",
        "\n",
        "EVT_FUEL : Describes how the vegetation typically behaves as fuel source in wildfire contexts\n",
        "\n",
        "EVT_FUEL_N : numeric counterpart to EVT_FUEL\n",
        "\n",
        "EVT_LF : lifeform classification\n",
        "\n",
        "EVT_PHYS : Describing physical structure of vegetation (e.g., forest, woodland, shrubland)\n",
        "\n",
        "EVT_GP : numeric code representing vegetation group that aggregates multiple EVT classes into categories \n",
        "\n",
        "EVT_GP_N  : Descriptive name\n",
        "\n",
        "EVT_CLASS : higher-level vegetation class grouping EVT types into generalized categories\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DtypeWarning: Columns (0: satellite, 1: instrument, 2: daynight, 3: timestamp_utc) have mixed types. Specify dtype option on import or set low_memory=False.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(861445, 46)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"master_final.csv\")\n",
        "df.shape"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dsml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
